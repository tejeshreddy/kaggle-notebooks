# [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)

## Table of Contents
- [Overview](#overview)
- [Notebooks](#notebooks)
- [Tools and Libraries Used](#tools-and-libraries-used)
- [Models and Eval Metrics](#models-and-eval-metrics)
- [Future Scope](#future-scope)

## Overview
This is an IPython Notebook for the Kaggle competition, Titanic Machine Learning From Disaster. This [Notebook](titanic-analysis.ipynb) aims at performing EDA(explanatory data analysis) on the dataset along with generating a [submission.csv](catboost_submission.csv) by running several classifier model.

## Notebooks
- [EDA and Model Building](titanic-analysis.ipynb)

## Tools and Libraries Used
- sklearn
- catboost

## Models and Eval Metrics
| Model | Accuracy Score |
| :-: | :-: |
| Decision Tree ||
| Gradient Boosting Trees ||
| CatBoost ||
| Logistic Regression ||
| Naive Bayes ||
| Linear SVC ||
| Stochastic Gradient Decent ||

## Future Scope
- Hypertune catboost model to enhance accuracy
- Use keras to build NN

<!-- 







## [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)
This is an IPython Notebook for the Kaggle competition, Titanic Machine Learning From Disaster. This [Notebook](titanic-analysis.ipynb) aims at performing EDA(explanatory data analysis) on the dataset along with generating a [submission.csv](catboost_submission.csv) by running several classifier model.

## Goals for this project

### Data Handling
- Importing Data with Pandas
- Cleaning Data
- Exploring Data through Visualizations with Matplotlib and Seaborn

### Data Analysis
Training on supervised ML models like:
- Decision Tree
- Gradient Boosting Trees
- CatBoost
- KNN
- Logistic Regression
- Naive Bayes
- Linear SVC
- Stochastic Gradient Decent

### Valuation of the Analysis
- Cross validation metrics validated on local
- Submiter output of the CatBoost(accuracy: 81.66%) results from the IPython Notebook to Kaggle -->
